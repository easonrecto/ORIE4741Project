{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load clean, processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"df3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111009, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## array of strings, 1 x n \n",
    "descriptions = df[\"description\"].tolist()\n",
    "\n",
    "## array of ints, 1 x n\n",
    "prices = df[\"price\"].tolist()\n",
    "\n",
    "## array of ints, 1 x n\n",
    "points = df[\"points\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Feature Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## must download vader_lexicon for vader sentiment algorithm\n",
    "nltk.download_shell()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis algorithms. Each takes a single string as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createDict():\n",
    "    sentiment_dictionary = {}\n",
    "    for line in open('afinn_dict.txt'):\n",
    "        word,score = line.split('\\t')\n",
    "        sentiment_dictionary[word] = int(score)\n",
    "    return sentiment_dictionary\n",
    "sentiment_dictionary = createDict()\n",
    "\n",
    "def sentimentAfinn(sentence):\n",
    "    '''\n",
    "    AFINN is a dictionary of polarity scores [-5,5] by word.\n",
    "    This algorithm sums the scores for each word in the sentence, then\n",
    "    classifies the entire sentence based on the sum's sign.\n",
    "    '''\n",
    "    sentence_tokens = sentence.split(' ')\n",
    "    score = 0\n",
    "    for token in sentence_tokens:\n",
    "        score += sentiment_dictionary.get(token,0)\n",
    "    ## if we want raw integer, not scaled\n",
    "    return score\n",
    "    ## if we want it to be scaled between -1 and 1\n",
    "    # return np.sign(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentim_int_analyzer = SentimentIntensityAnalyzer()\n",
    "def sentimentVader(sentence):    \n",
    "    '''\n",
    "    Sentiment object contains { 'pos', 'neg', 'neu', 'compound' } where pos+neg+neu=1, compound is [-1,1].\n",
    "    This algorithm returns the 'pos', 'neu', 'neg' values\n",
    "    '''\n",
    "    ss = sentim_int_analyzer.polarity_scores(sentence)\n",
    "    return [ss['pos'], ss['neu'], ss['neg']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create sentiment feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentiments_sums = list(map(sentimentAfinn,descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentiments_probs = list(map(sentimentVader,descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentiments_negs = sentiment_probs[:,1].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the transformed feature vectors to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('sentiment_sums.json', 'w') as outfile:\n",
    "    json.dump(sentiments_sums, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('sentiment_probabilities.json', 'w') as outfile:\n",
    "    json.dump(sentiments_probs, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('sentiment_probabilities_negative.json', 'w') as outfile:\n",
    "    json.dump(sentiments_negs, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('sentiment_sums.json') as json_data:\n",
    "    sentiment_sums = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('sentiment_probabilities.json') as json_data:\n",
    "    sentiment_probs = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentiment_probs = np.matrix(sentiment_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plotting sentiment sums against points\n",
    "plt.plot(sentiment_sums, points, 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plotting sentiment sums against prices\n",
    "plt.plot(sentiment_sums, prices, 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plotting positive probabilities against points\n",
    "plt.plot(sentiment_probs[:,0], points, 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plotting positive probabilities against price\n",
    "plt.plot(sentiment_probs[:,0], prices, 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plotting neutral probabilities against points\n",
    "plt.plot(sentiment_probs[:,1], points, 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plotting neutral probabilities against prices\n",
    "plt.plot(sentiment_probs[:,1], prices, 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plotting negative probabilities against points\n",
    "plt.plot(sentiment_probs[:,2], points, 'ro')\n",
    "plt.xlabel('probability that description has negative sentiment')\n",
    "plt.ylabel('points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plotting negative probabilities against price\n",
    "plt.plot(sentiment_probs[:,2], prices, 'ro')\n",
    "plt.xlabel('probability that description has negative sentiment')\n",
    "plt.ylabel('price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> d\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> stopwords\n",
      "    Downloading package stopwords to C:\\Users\\cit-\n",
      "        labs\\AppData\\Roaming\\nltk_data...\n",
      "      Unzipping corpora\\stopwords.zip.\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> q\n"
     ]
    }
   ],
   "source": [
    "## must download stopwords for word embeddings\n",
    "nltk.download_shell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 20\n",
    "embedding_depth = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# will remove stopwords from sentences before word embedding transformation\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend(string.punctuation)\n",
    "stopwords.append('')\n",
    "stopwords.remove('not')\n",
    "stopwords.remove('no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.WhitespaceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mapping 3 billion words to embedded vectors, obtained from Google\n",
    "embedding_dict = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(word):\n",
    "    \"\"\"\n",
    "    Returns a lower cased version of a word and removes\n",
    "    punctuation.\n",
    "    \"\"\"\n",
    "    return word.lower().strip(string.punctuation)\n",
    "    \n",
    "def tokenize(sentence):\n",
    "    '''\n",
    "    Converts string sentence into list of words. Strips punctuation,\n",
    "    removes long words, makes lower case.\n",
    "    '''\n",
    "    tokens = []\n",
    "    for token in tokenizer.tokenize(sentence):\n",
    "        word = normalize(token)\n",
    "        if word not in stopwords and len(word)<MAX_SEQUENCE_LENGTH:\n",
    "            tokens.append(word)\n",
    "    return tokens\n",
    "\n",
    "def to_vector(word):\n",
    "    \"\"\"\n",
    "    Convert a word to vector if we have a vector representation.\n",
    "    \"\"\"\n",
    "    if word in embedding_dict:\n",
    "        return embedding_dict[word]\n",
    "    else:\n",
    "        return np.zeros(300, dtype=float)\n",
    "\n",
    "def sentence_to_vec(sentence):\n",
    "    '''\n",
    "    Converts sentence into a float matrix = list of word vectors.\n",
    "    '''\n",
    "    import numpy as np\n",
    "    z = np.zeros([300,])\n",
    "    sentence_matrix = [to_vector(token) for token in tokenize(sentence)]\n",
    "    padding = MAX_SEQUENCE_LENGTH - len(sentence_matrix)\n",
    "    if padding >= 0:\n",
    "        for i in range(0,padding):\n",
    "            sentence_matrix.append(z)\n",
    "    else:\n",
    "        sentence_matrix = sentence_matrix[:MAX_SEQUENCE_LENGTH]\n",
    "    return np.array(sentence_matrix)\n",
    "\n",
    "def sentences_to_vecs(sentences):\n",
    "    '''\n",
    "    Converts list of sentences into list of sentence embeddings, \n",
    "    each embedding is MAX_SEQUENCE_LENGTH x 300.\n",
    "    '''\n",
    "    training_data = [sentence_to_vec(sentence) for sentence in sentences]\n",
    "    return np.array(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# map sentences from wine descriptions to word embedding vectors\n",
    "embedded_sentences = sentences_to_vecs(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(111009, 20, 300)\n"
     ]
    }
   ],
   "source": [
    "# dimensions of each embedded sentences should be 20 x 300\n",
    "print(embedded_sentences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('embedded.npy', embedded_sentences) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedded_sentences = np.load('embedded.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(embedded_sentences, prices, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Activation, Dot, Embedding, Conv1D, MaxPooling1D, Merge, Highway, LSTM, Dense, Dropout, Reshape, ActivityRegularization, Input\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from keras import regularizers\n",
    "from keras.constraints import non_neg\n",
    "from keras.initializers import TruncatedNormal\n",
    "\n",
    "def descriptions_to_price(embedding_depth, max_seq_len):\n",
    "    '''\n",
    "    Returns a model for sentiment analysis algorithm weight learning.\n",
    "    Model inputs: array of sentence vectors (dimensions: sentences x 22 x 300), \n",
    "                  array of sentiment guesses for each algorithm (dimensions: sentences x algorithms x 3)\n",
    "          outputs: array of sentiment answers (dimensions: sentences x 3)\n",
    "    '''\n",
    "    # input layers\n",
    "    main_input = Input(shape=(max_seq_len,embedding_depth), dtype='float32', name='main_input')\n",
    "    \n",
    "    lstm = LSTM(300)\n",
    "    lstm_main = lstm(main_input)\n",
    "    \n",
    "    dense_nn = Dense(1, activation='linear')\n",
    "    dense_nn_main = dense_nn(lstm_main)\n",
    "\n",
    "        \n",
    "    # main model\n",
    "    main_model = Model(inputs=[main_input], outputs=[dense_nn_main])\n",
    "    op = RMSprop(lr=0.00007)\n",
    "    main_model.compile(optimizer=op, loss='mean_absolute_error')\n",
    "    \n",
    "    return main_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "main_input (InputLayer)      (None, 20, 300)           0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 300)               721200    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 721,501\n",
      "Trainable params: 721,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#build/compile model\n",
    "model = descriptions_to_price(embedding_depth, MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "#view model summary\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 84921 samples, validate on 14987 samples\n",
      "Epoch 1/20\n",
      "84921/84921 [==============================] - 93s 1ms/step - loss: 15.5911 - val_loss: 15.3171\n",
      "Epoch 2/20\n",
      "84921/84921 [==============================] - 94s 1ms/step - loss: 15.3645 - val_loss: 15.1478\n",
      "Epoch 3/20\n",
      "84921/84921 [==============================] - 94s 1ms/step - loss: 15.2139 - val_loss: 15.8349\n",
      "Epoch 4/20\n",
      "84921/84921 [==============================] - 92s 1ms/step - loss: 15.0989 - val_loss: 15.2798\n",
      "Epoch 5/20\n",
      "84921/84921 [==============================] - 91s 1ms/step - loss: 15.0047 - val_loss: 15.0580\n",
      "Epoch 6/20\n",
      "84921/84921 [==============================] - 91s 1ms/step - loss: 14.9166 - val_loss: 15.0740\n",
      "Epoch 7/20\n",
      "84921/84921 [==============================] - 91s 1ms/step - loss: 14.8527 - val_loss: 15.0558\n",
      "Epoch 8/20\n",
      "84921/84921 [==============================] - 91s 1ms/step - loss: 14.7892 - val_loss: 14.7324\n",
      "Epoch 9/20\n",
      "84921/84921 [==============================] - 91s 1ms/step - loss: 14.7108 - val_loss: 14.8536\n",
      "Epoch 10/20\n",
      "84921/84921 [==============================] - 91s 1ms/step - loss: 14.6585 - val_loss: 14.5886\n",
      "Epoch 11/20\n",
      "84921/84921 [==============================] - 91s 1ms/step - loss: 14.5932 - val_loss: 14.6884\n",
      "Epoch 12/20\n",
      "84921/84921 [==============================] - 91s 1ms/step - loss: 14.5215 - val_loss: 14.6362\n",
      "Epoch 13/20\n",
      "84921/84921 [==============================] - 91s 1ms/step - loss: 14.4663 - val_loss: 14.5631\n",
      "Epoch 14/20\n",
      "84921/84921 [==============================] - 91s 1ms/step - loss: 14.4086 - val_loss: 14.4385\n",
      "Epoch 15/20\n",
      "84921/84921 [==============================] - 91s 1ms/step - loss: 14.3511 - val_loss: 14.5168\n",
      "Epoch 16/20\n",
      "84921/84921 [==============================] - 91s 1ms/step - loss: 14.2806 - val_loss: 14.3211\n",
      "Epoch 17/20\n",
      "84921/84921 [==============================] - 91s 1ms/step - loss: 14.2264 - val_loss: 14.3506\n",
      "Epoch 18/20\n",
      "84921/84921 [==============================] - 90s 1ms/step - loss: 14.1801 - val_loss: 14.5217\n",
      "Epoch 19/20\n",
      "84921/84921 [==============================] - 91s 1ms/step - loss: 14.1158 - val_loss: 14.3731\n",
      "Epoch 20/20\n",
      "84921/84921 [==============================] - 91s 1ms/step - loss: 14.0487 - val_loss: 14.1955\n"
     ]
    }
   ],
   "source": [
    "# train model on train set\n",
    "history = model.fit([train_x], train_y, batch_size=128, epochs=20, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use the model to make predictions on unseen data\n",
    "predicted_prices = model.predict([test_x])\n",
    "preds = predicted_prices.astype(type('float', (float,), {}))\n",
    "preds2 = list(preds.reshape(-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('nn_true.json', 'w') as outfile:\n",
    "    json.dump(test_y, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('nn_predictions.json', 'w') as outfile:\n",
    "    json.dump(preds2, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11101/11101 [==============================] - 4s 400us/step\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_x, test_y, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.1787605293\n"
     ]
    }
   ],
   "source": [
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
